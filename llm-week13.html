<!DOCTYPE html>
<html lang="zh-TW">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Week 13 - LLM 訓練與優化 | LLM 與 AI/ML 課程</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', 'Microsoft JhengHei', Arial, sans-serif;
            background: #f8f9fa;
            color: #1a1a1a;
            line-height: 1.7;
        }

        .container {
            max-width: 1000px;
            margin: 0 auto;
            padding: 20px;
        }

        .nav {
            background: white;
            padding: 16px 28px;
            border-radius: 8px;
            margin-bottom: 32px;
            display: flex;
            justify-content: space-between;
            align-items: center;
            box-shadow: 0 2px 8px rgba(0,0,0,0.08);
            border: 1px solid #e0e0e0;
        }

        .nav a {
            color: #3498db;
            text-decoration: none;
            font-weight: 500;
            padding: 8px 16px;
            border-radius: 4px;
            transition: all 0.2s ease;
        }

        .nav a:hover {
            background: #ecf0f1;
            color: #2980b9;
        }

        .nav span {
            color: #6c757d;
            font-weight: 500;
        }

        header {
            background: linear-gradient(to right, #2c3e50, #34495e);
            color: white;
            padding: 56px 40px;
            border-radius: 8px;
            margin-bottom: 32px;
            box-shadow: 0 4px 12px rgba(0,0,0,0.15);
            border-bottom: 3px solid #3498db;
        }

        header h1 {
            font-size: 2.2em;
            margin-bottom: 12px;
            font-weight: 700;
        }

        header .subtitle {
            font-size: 1.05em;
            opacity: 0.9;
            font-weight: 400;
        }

        .content {
            background: white;
            border-radius: 8px;
            padding: 48px;
            margin-bottom: 32px;
            box-shadow: 0 2px 8px rgba(0,0,0,0.08);
            border: 1px solid #e0e0e0;
        }

        .content h2 {
            color: #1a1a1a;
            font-size: 1.9em;
            margin-bottom: 24px;
            padding-bottom: 12px;
            border-bottom: 2px solid #3498db;
            font-weight: 700;
        }

        .content h3 {
            color: #2c3e50;
            font-size: 1.5em;
            margin: 36px 0 16px 0;
            font-weight: 600;
        }

        .content h4 {
            color: #34495e;
            font-size: 1.2em;
            margin: 24px 0 12px 0;
            font-weight: 600;
        }

        .content p {
            margin-bottom: 16px;
            line-height: 1.8;
            color: #4a4a4a;
        }

        pre[class*="language-"] {
            margin: 24px 0;
            border-radius: 6px;
            border: 1px solid #d0d7de;
            box-shadow: 0 1px 3px rgba(0,0,0,0.08);
        }

        code[class*="language-"] {
            font-family: 'SF Mono', Monaco, 'Cascadia Code', 'Roboto Mono', Consolas, 'Courier New', monospace;
            font-size: 0.92em;
            line-height: 1.6;
        }

        .note-box {
            background: #e3f2fd;
            border-left: 4px solid #2196f3;
            padding: 20px 24px;
            margin: 24px 0;
            border-radius: 4px;
        }

        .note-box h4 {
            color: #1565c0;
            margin-top: 0;
            margin-bottom: 12px;
            font-size: 1.1em;
        }

        .warning-box {
            background: #fff3e0;
            border-left: 4px solid #ff9800;
            padding: 20px 24px;
            margin: 24px 0;
            border-radius: 4px;
        }

        .warning-box h4 {
            color: #e65100;
            margin-top: 0;
            margin-bottom: 12px;
            font-size: 1.1em;
        }

        .tip-box {
            background: #f1f8e9;
            border-left: 4px solid #8bc34a;
            padding: 20px 24px;
            margin: 24px 0;
            border-radius: 4px;
        }

        .tip-box h4 {
            color: #558b2f;
            margin-top: 0;
            margin-bottom: 12px;
            font-size: 1.1em;
        }

        .concept-box {
            background: #f3e5f5;
            border-left: 4px solid #9c27b0;
            padding: 20px 24px;
            margin: 24px 0;
            border-radius: 4px;
        }

        .concept-box h4 {
            color: #6a1b9a;
            margin-top: 0;
            margin-bottom: 12px;
            font-size: 1.1em;
        }

        .exercise-section {
            background: #fafafa;
            border: 1px solid #e0e0e0;
            padding: 32px;
            margin: 32px 0;
            border-radius: 8px;
        }

        .exercise-section h3 {
            color: #2c3e50;
            margin-top: 0;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin: 24px 0;
            background: white;
        }

        table th,
        table td {
            padding: 14px 16px;
            text-align: left;
            border: 1px solid #dee2e6;
        }

        table th {
            background: #2c3e50;
            color: white;
            font-weight: 600;
        }

        table tr:nth-child(even) {
            background: #f8f9fa;
        }

        ul, ol {
            margin-left: 28px;
            margin-top: 12px;
            margin-bottom: 16px;
        }

        li {
            margin: 10px 0;
            line-height: 1.7;
            color: #4a4a4a;
        }

        .nav-buttons {
            display: flex;
            justify-content: space-between;
            margin-top: 48px;
            gap: 16px;
        }

        .btn {
            display: inline-block;
            padding: 12px 28px;
            background: #3498db;
            color: white;
            text-decoration: none;
            border-radius: 6px;
            font-weight: 500;
            transition: all 0.2s ease;
            border: none;
        }

        .btn:hover {
            background: #2980b9;
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(52, 152, 219, 0.3);
        }

        .btn-secondary {
            background: #95a5a6;
        }

        .btn-secondary:hover {
            background: #7f8c8d;
            box-shadow: 0 4px 12px rgba(149, 165, 166, 0.3);
        }

        .highlight {
            background: #fff9c4;
            padding: 2px 6px;
            border-radius: 3px;
            font-weight: 500;
        }

        footer {
            text-align: center;
            padding: 32px 0;
            color: #6c757d;
            margin-top: 48px;
            border-top: 1px solid #dee2e6;
        }

        footer p {
            margin: 8px 0;
        }

        @media (max-width: 768px) {
            .container {
                padding: 12px;
            }

            .content {
                padding: 24px 20px;
            }

            header {
                padding: 32px 24px;
            }

            header h1 {
                font-size: 1.8em;
            }

            .nav-buttons {
                flex-direction: column;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <div class="nav">
            <a href="llm-week12.html">Week 12</a>
            <span>Week 13 / 16</span>
            <a href="llm-week14.html">Week 14</a>
        </div>

        <header>
            <h1>Week 13 - LLM 訓練與優化</h1>
            <div class="subtitle">分散式訓練、混合精度與模型量化</div>
        </header>

        <div class="content">
            <h2>本週學習內容</h2>
            <ul>
                <li>分散式訓練（Data Parallel、Model Parallel、Pipeline Parallel）</li>
                <li>混合精度訓練（FP16、BF16）</li>
                <li>梯度累積與梯度檢查點</li>
                <li>模型量化（INT8、INT4、GPTQ）</li>
                <li>推理優化（KV Cache、Flash Attention）</li>
            </ul>

            <div class="concept-box">
                <h4>為什麼需要優化？</h4>
                <p>大型語言模型訓練面臨巨大挑戰：</p>
                <ul>
                    <li><strong>記憶體限制</strong> - GPT-3 (175B) 需要約 700GB 顯存</li>
                    <li><strong>計算成本</strong> - 訓練成本可達數百萬美元</li>
                    <li><strong>訓練時間</strong> - 可能需要數週甚至數月</li>
                    <li><strong>推理延遲</strong> - 需要快速回應使用者</li>
                </ul>
            </div>
        </div>

        <div class="content">
            <h2>1. 分散式訓練</h2>
            
            <h3>Data Parallelism（資料並行）</h3>
            <p><span class="highlight">資料並行</span>是最簡單的分散式訓練方式：在多個 GPU 上複製模型，分割資料。</p>

            <pre><code class="language-python">import torch
import torch.nn as nn
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP

class DataParallelTraining:
    """
    資料並行訓練
    
    每個 GPU 有完整模型副本
    資料被分割到不同 GPU
    """
    def __init__(self, model, rank, world_size):
        self.rank = rank
        self.world_size = world_size
        
        # 初始化分散式環境
        dist.init_process_group(
            backend='nccl',  # NVIDIA GPU 使用 nccl
            init_method='env://',
            world_size=world_size,
            rank=rank
        )
        
        # 設定 device
        self.device = torch.device(f'cuda:{rank}')
        torch.cuda.set_device(self.device)
        
        # 將模型包裝為 DDP
        self.model = model.to(self.device)
        self.model = DDP(self.model, device_ids=[rank])
    
    def train_step(self, data, labels, optimizer, criterion):
        """
        訓練一步
        
        梯度會自動在所有 GPU 間同步
        """
        data = data.to(self.device)
        labels = labels.to(self.device)
        
        # 前向傳播
        outputs = self.model(data)
        loss = criterion(outputs, labels)
        
        # 反向傳播
        optimizer.zero_grad()
        loss.backward()
        
        # 梯度會自動同步（DDP 處理）
        optimizer.step()
        
        return loss.item()
    
    def cleanup(self):
        """清理分散式環境"""
        dist.destroy_process_group()

# 使用範例
"""
# 在每個進程中運行
def main(rank, world_size):
    model = YourModel()
    trainer = DataParallelTraining(model, rank, world_size)
    
    optimizer = torch.optim.Adam(model.parameters())
    criterion = nn.CrossEntropyLoss()
    
    for epoch in range(epochs):
        for data, labels in dataloader:
            loss = trainer.train_step(data, labels, optimizer, criterion)
    
    trainer.cleanup()

# 啟動多進程
import torch.multiprocessing as mp
mp.spawn(main, args=(world_size,), nprocs=world_size)
"""

print("資料並行訓練範例")
print("優點：簡單、適合大批次訓練")
print("缺點：每個 GPU 需要存放完整模型")
</code></pre>

            <h3>Model Parallelism（模型並行）</h3>
            <p>當模型太大無法放入單個 GPU 時，需要將模型分割到多個 GPU。</p>

            <pre><code class="language-python">class ModelParallelTransformer(nn.Module):
    """
    模型並行的 Transformer
    
    將不同層分配到不同 GPU
    """
    def __init__(self, num_layers=12, d_model=768, num_heads=12):
        super(ModelParallelTransformer, self).__init__()
        
        # 將層分配到不同 GPU
        layers_per_gpu = num_layers // 2
        
        # GPU 0: 前半層
        self.layers_gpu0 = nn.ModuleList([
            nn.TransformerEncoderLayer(d_model, num_heads, batch_first=True)
            for _ in range(layers_per_gpu)
        ]).to('cuda:0')
        
        # GPU 1: 後半層
        self.layers_gpu1 = nn.ModuleList([
            nn.TransformerEncoderLayer(d_model, num_heads, batch_first=True)
            for _ in range(layers_per_gpu)
        ]).to('cuda:1')
        
        # 輸出層在 GPU 1
        self.output = nn.Linear(d_model, 50000).to('cuda:1')
    
    def forward(self, x):
        """
        前向傳播
        
        資料在 GPU 之間傳輸
        """
        # GPU 0: 前半層
        x = x.to('cuda:0')
        for layer in self.layers_gpu0:
            x = layer(x)
        
        # 傳輸到 GPU 1
        x = x.to('cuda:1')
        
        # GPU 1: 後半層
        for layer in self.layers_gpu1:
            x = layer(x)
        
        # 輸出
        x = self.output(x)
        
        return x

# 測試模型並行
# model_parallel = ModelParallelTransformer()
print("\n模型並行範例")
print("優點：可以訓練超大模型")
print("缺點：GPU 利用率低（流水線氣泡）")
</code></pre>

            <h3>Pipeline Parallelism（流水線並行）</h3>
            <p>改進模型並行，透過微批次提高 GPU 利用率。</p>

            <pre><code class="language-python">class PipelineParallelism:
    """
    流水線並行
    
    將批次分成微批次，流水線執行
    """
    def __init__(self, model_stages, num_microbatches=4):
        """
        model_stages: 模型的不同階段（在不同 GPU 上）
        num_microbatches: 微批次數量
        """
        self.stages = model_stages
        self.num_microbatches = num_microbatches
    
    def forward_backward(self, inputs, labels):
        """
        前向和反向傳播（流水線）
        
        微批次 1: GPU0 前向 -> GPU1 前向 -> GPU1 反向 -> GPU0 反向
        微批次 2: GPU0 前向 -> GPU1 前向 -> GPU1 反向 -> GPU0 反向
        ...
        """
        batch_size = inputs.size(0)
        microbatch_size = batch_size // self.num_microbatches
        
        # 分割成微批次
        microbatches = [
            inputs[i*microbatch_size:(i+1)*microbatch_size]
            for i in range(self.num_microbatches)
        ]
        
        label_microbatches = [
            labels[i*microbatch_size:(i+1)*microbatch_size]
            for i in range(self.num_microbatches)
        ]
        
        # 流水線執行
        activations = []
        losses = []
        
        # 前向傳播
        for microbatch in microbatches:
            x = microbatch
            for stage in self.stages:
                x = stage(x)
            activations.append(x)
        
        # 計算損失
        for act, label_mb in zip(activations, label_microbatches):
            loss = nn.CrossEntropyLoss()(act, label_mb)
            losses.append(loss)
        
        # 反向傳播
        total_loss = sum(losses) / len(losses)
        
        return total_loss

print("\n流水線並行範例")
print("優點：提高 GPU 利用率")
print("缺點：需要仔細調整微批次大小")
</code></pre>

            <div class="note-box">
                <h4>並行策略比較</h4>
                <table>
                    <tr>
                        <th>策略</th>
                        <th>記憶體</th>
                        <th>計算效率</th>
                        <th>通訊開銷</th>
                        <th>適用場景</th>
                    </tr>
                    <tr>
                        <td>Data Parallel</td>
                        <td>高</td>
                        <td>高</td>
                        <td>中</td>
                        <td>模型能放入單 GPU</td>
                    </tr>
                    <tr>
                        <td>Model Parallel</td>
                        <td>低</td>
                        <td>低</td>
                        <td>高</td>
                        <td>模型太大</td>
                    </tr>
                    <tr>
                        <td>Pipeline Parallel</td>
                        <td>低</td>
                        <td>中</td>
                        <td>中</td>
                        <td>平衡記憶體和效率</td>
                    </tr>
                    <tr>
                        <td>混合並行</td>
                        <td>低</td>
                        <td>高</td>
                        <td>中</td>
                        <td>超大規模訓練</td>
                    </tr>
                </table>
            </div>
        </div>

        <div class="content">
            <h2>2. 混合精度訓練</h2>
            
            <h3>FP16 與 FP32</h3>
            <p><span class="highlight">混合精度訓練</span>使用 FP16 加速計算，同時保持 FP32 主權重以確保精度。</p>

            <pre><code class="language-python">import torch
from torch.cuda.amp import autocast, GradScaler

class MixedPrecisionTrainer:
    """
    混合精度訓練
    
    使用 FP16 進行前向和反向傳播
    使用 FP32 儲存主權重
    """
    def __init__(self, model, optimizer):
        self.model = model
        self.optimizer = optimizer
        
        # 梯度縮放器（處理數值穩定性）
        self.scaler = GradScaler()
    
    def train_step(self, data, labels, criterion):
        """
        混合精度訓練步驟
        """
        # 使用自動混合精度
        with autocast():
            # 前向傳播（FP16）
            outputs = self.model(data)
            loss = criterion(outputs, labels)
        
        # 反向傳播
        self.optimizer.zero_grad()
        
        # 縮放損失（防止梯度下溢）
        self.scaler.scale(loss).backward()
        
        # 取消縮放並更新
        self.scaler.step(self.optimizer)
        self.scaler.update()
        
        return loss.item()
    
    def get_memory_savings(self):
        """計算記憶體節省"""
        # FP16 使用一半記憶體
        return "約 50% 記憶體節省"

# 測試混合精度訓練
model = nn.Linear(1024, 1024).cuda()
optimizer = torch.optim.Adam(model.parameters())
trainer = MixedPrecisionTrainer(model, optimizer)

print("混合精度訓練")
print(f"記憶體節省: {trainer.get_memory_savings()}")
print("速度提升: 約 2-3 倍（在支援 Tensor Core 的 GPU 上）")

# 訓練範例
data = torch.randn(32, 1024).cuda()
labels = torch.randn(32, 1024).cuda()
criterion = nn.MSELoss()

loss = trainer.train_step(data, labels, criterion)
print(f"訓練損失: {loss:.4f}")
</code></pre>

            <h3>BF16（Brain Float 16）</h3>
            <p>BF16 有更大的動態範圍，適合訓練大型模型。</p>

            <div class="concept-box">
                <h4>數值格式比較</h4>
                <ul>
                    <li><strong>FP32</strong> - 1 符號位 + 8 指數位 + 23 尾數位</li>
                    <li><strong>FP16</strong> - 1 符號位 + 5 指數位 + 10 尾數位</li>
                    <li><strong>BF16</strong> - 1 符號位 + 8 指數位 + 7 尾數位</li>
                </ul>
                <p>BF16 與 FP32 有相同的指數範圍，減少溢出風險。</p>
            </div>

            <pre><code class="language-python"># 使用 BF16
"""
# PyTorch 1.10+ 支援
with torch.autocast(device_type='cuda', dtype=torch.bfloat16):
    output = model(input)
    loss = criterion(output, target)

loss.backward()
optimizer.step()
"""

print("\nBF16 優勢:")
print("1. 動態範圍大，不易溢出")
print("2. 不需要損失縮放")
print("3. 適合大型模型訓練")
print("4. Google TPU 原生支援")
</code></pre>
        </div>

        <div class="content">
            <h2>3. 梯度累積與梯度檢查點</h2>
            
            <h3>梯度累積</h3>
            <p><span class="highlight">梯度累積</span>允許使用更大的有效批次大小，而不增加記憶體使用。</p>

            <pre><code class="language-python">class GradientAccumulation:
    """
    梯度累積
    
    累積多個小批次的梯度，再一次更新
    """
    def __init__(self, model, optimizer, accumulation_steps=4):
        self.model = model
        self.optimizer = optimizer
        self.accumulation_steps = accumulation_steps
    
    def train_epoch(self, dataloader, criterion):
        """
        訓練一個 epoch
        """
        self.model.train()
        total_loss = 0
        
        for i, (data, labels) in enumerate(dataloader):
            # 前向傳播
            outputs = self.model(data)
            loss = criterion(outputs, labels)
            
            # 標準化損失（因為要累積多個批次）
            loss = loss / self.accumulation_steps
            
            # 反向傳播（累積梯度）
            loss.backward()
            
            # 每 accumulation_steps 步更新一次
            if (i + 1) % self.accumulation_steps == 0:
                self.optimizer.step()
                self.optimizer.zero_grad()
            
            total_loss += loss.item() * self.accumulation_steps
        
        return total_loss / len(dataloader)

# 示例
model = nn.Linear(128, 10)
optimizer = torch.optim.SGD(model.parameters(), lr=0.01)
trainer = GradientAccumulation(model, optimizer, accumulation_steps=4)

print("梯度累積")
print("有效批次大小 = 實際批次大小 × 累積步數")
print("例如：批次 32 × 累積 4 步 = 有效批次 128")
print("\n優點：")
print("- 可以使用更大的批次大小")
print("- 不增加記憶體使用")
print("缺點：")
print("- 訓練速度稍慢（更多前向傳播）")
</code></pre>

            <h3>梯度檢查點（Gradient Checkpointing）</h3>
            <p>透過重新計算來節省記憶體，以時間換空間。</p>

            <pre><code class="language-python">from torch.utils.checkpoint import checkpoint

class CheckpointedTransformer(nn.Module):
    """
    使用梯度檢查點的 Transformer
    
    不儲存所有中間激活，反向時重新計算
    """
    def __init__(self, num_layers=12, d_model=768, num_heads=12):
        super(CheckpointedTransformer, self).__init__()
        
        self.layers = nn.ModuleList([
            nn.TransformerEncoderLayer(d_model, num_heads, batch_first=True)
            for _ in range(num_layers)
        ])
    
    def forward(self, x):
        """
        前向傳播
        
        使用 checkpoint 包裝每一層
        """
        for layer in self.layers:
            # 使用檢查點（不儲存中間激活）
            x = checkpoint(layer, x, use_reentrant=False)
        
        return x

# 測試梯度檢查點
model_checkpointed = CheckpointedTransformer()
model_normal = CheckpointedTransformer()

print("\n梯度檢查點")
print("記憶體節省: 約 O(√n) 其中 n 是層數")
print("時間開銷: 約 20-30% 增加")
print("\n適用場景:")
print("- 記憶體受限")
print("- 模型很深")
print("- 批次大小更重要")
</code></pre>

            <div class="tip-box">
                <h4>記憶體優化技巧</h4>
                <ul>
                    <li><strong>混合精度</strong> - 50% 記憶體節省</li>
                    <li><strong>梯度累積</strong> - 減少批次大小</li>
                    <li><strong>梯度檢查點</strong> - 節省激活記憶體</li>
                    <li><strong>ZeRO</strong> - 分散優化器狀態</li>
                    <li><strong>組合使用</strong> - 可以疊加效果</li>
                </ul>
            </div>
        </div>

        <div class="content">
            <h2>4. 模型量化</h2>
            
            <h3>量化基礎</h3>
            <p><span class="highlight">量化</span>將高精度權重轉換為低精度，大幅減少模型大小和推理時間。</p>

            <div class="concept-box">
                <h4>量化類型</h4>
                <ul>
                    <li><strong>量化感知訓練（QAT）</strong> - 訓練時模擬量化</li>
                    <li><strong>訓練後量化（PTQ）</strong> - 訓練後直接量化</li>
                    <li><strong>動態量化</strong> - 推理時動態量化激活</li>
                    <li><strong>靜態量化</strong> - 預先校準量化參數</li>
                </ul>
            </div>

            <pre><code class="language-python">import torch.quantization as quantization

class ModelQuantization:
    """
    模型量化工具
    """
    @staticmethod
    def dynamic_quantization(model):
        """
        動態量化（最簡單）
        
        權重量化為 INT8
        激活在推理時動態量化
        """
        quantized_model = quantization.quantize_dynamic(
            model,
            {nn.Linear, nn.LSTM},  # 要量化的層類型
            dtype=torch.qint8
        )
        
        return quantized_model
    
    @staticmethod
    def static_quantization(model, calibration_data):
        """
        靜態量化
        
        需要校準資料來確定量化參數
        """
        # 設定量化配置
        model.qconfig = quantization.get_default_qconfig('fbgemm')
        
        # 準備模型
        quantization.prepare(model, inplace=True)
        
        # 校準（收集統計資訊）
        model.eval()
        with torch.no_grad():
            for data in calibration_data:
                model(data)
        
        # 轉換為量化模型
        quantized_model = quantization.convert(model, inplace=False)
        
        return quantized_model
    
    @staticmethod
    def qat_training(model, train_loader, epochs=10):
        """
        量化感知訓練
        
        訓練時模擬量化效果
        """
        # 設定 QAT 配置
        model.qconfig = quantization.get_default_qat_qconfig('fbgemm')
        
        # 準備 QAT
        quantization.prepare_qat(model, inplace=True)
        
        # 訓練
        optimizer = torch.optim.SGD(model.parameters(), lr=0.01)
        criterion = nn.CrossEntropyLoss()
        
        for epoch in range(epochs):
            model.train()
            for data, labels in train_loader:
                outputs = model(data)
                loss = criterion(outputs, labels)
                
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()
        
        # 轉換為量化模型
        model.eval()
        quantized_model = quantization.convert(model, inplace=False)
        
        return quantized_model

# 測試量化
original_model = nn.Sequential(
    nn.Linear(128, 256),
    nn.ReLU(),
    nn.Linear(256, 10)
)

# 動態量化
quantized = ModelQuantization.dynamic_quantization(original_model)

print("模型量化")
print(f"原始模型大小: {sum(p.numel() * 4 for p in original_model.parameters()) / 1e6:.2f} MB (FP32)")
print(f"量化模型大小: {sum(p.numel() for p in quantized.parameters()) / 1e6:.2f} MB (INT8)")
print("壓縮比: 約 4x")
print("速度提升: 2-4x（CPU 推理）")
</code></pre>

            <h3>進階量化：GPTQ 與 AWQ</h3>
            <pre><code class="language-python">class AdvancedQuantization:
    """
    進階量化技術
    
    GPTQ: 針對生成式模型優化
    AWQ: 激活感知權重量化
    """
    @staticmethod
    def gptq_quantization(model, calibration_data, bits=4):
        """
        GPTQ 量化
        
        逐層量化，最小化量化誤差
        """
        print(f"GPTQ {bits}-bit 量化")
        print("特點:")
        print("- 針對 Transformer 優化")
        print("- 支援 4-bit 甚至 3-bit")
        print("- 保持接近原始精度")
        print("- 適合大型語言模型")
        
        # 實際實作需要使用 auto-gptq 庫
        """
        from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig
        
        quantize_config = BaseQuantizeConfig(
            bits=bits,
            group_size=128,
            desc_act=False
        )
        
        quantized_model = AutoGPTQForCausalLM.from_pretrained(
            model,
            quantize_config=quantize_config
        )
        
        quantized_model.quantize(calibration_data)
        """
        
        return model  # 簡化返回
    
    @staticmethod
    def awq_quantization(model):
        """
        AWQ (Activation-aware Weight Quantization)
        
        根據激活重要性選擇性量化
        """
        print("\nAWQ 量化")
        print("特點:")
        print("- 保護重要通道")
        print("- 激活感知")
        print("- 零樣本量化（不需要校準資料）")
        print("- 4-bit 接近 FP16 精度")
        
        return model

# 展示進階量化
print("\n進階量化技術:")
adv_quant = AdvancedQuantization()
adv_quant.gptq_quantization(None, None, bits=4)
adv_quant.awq_quantization(None)
</code></pre>

            <div class="note-box">
                <h4>量化精度比較</h4>
                <table>
                    <tr>
                        <th>精度</th>
                        <th>模型大小</th>
                        <th>精度損失</th>
                        <th>速度提升</th>
                        <th>適用場景</th>
                    </tr>
                    <tr>
                        <td>FP32</td>
                        <td>100%</td>
                        <td>0%</td>
                        <td>1x</td>
                        <td>訓練</td>
                    </tr>
                    <tr>
                        <td>FP16</td>
                        <td>50%</td>
                        <td>< 0.1%</td>
                        <td>2-3x</td>
                        <td>訓練、推理</td>
                    </tr>
                    <tr>
                        <td>INT8</td>
                        <td>25%</td>
                        <td>< 1%</td>
                        <td>2-4x</td>
                        <td>推理</td>
                    </tr>
                    <tr>
                        <td>INT4</td>
                        <td>12.5%</td>
                        <td>1-3%</td>
                        <td>3-5x</td>
                        <td>推理（GPTQ/AWQ）</td>
                    </tr>
                </table>
            </div>
        </div>

        <div class="content">
            <h2>5. 推理優化</h2>
            
            <h3>KV Cache</h3>
            <p><span class="highlight">KV Cache</span> 快取注意力計算中的 Key 和 Value，避免重複計算。</p>

            <pre><code class="language-python">class KVCacheAttention(nn.Module):
    """
    帶 KV Cache 的注意力機制
    
    在自回歸生成中，之前的 K 和 V 可以被重用
    """
    def __init__(self, d_model, num_heads):
        super(KVCacheAttention, self).__init__()
        
        self.d_model = d_model
        self.num_heads = num_heads
        self.head_dim = d_model // num_heads
        
        self.q_proj = nn.Linear(d_model, d_model)
        self.k_proj = nn.Linear(d_model, d_model)
        self.v_proj = nn.Linear(d_model, d_model)
        self.out_proj = nn.Linear(d_model, d_model)
        
        # KV Cache
        self.k_cache = None
        self.v_cache = None
    
    def forward(self, x, use_cache=True):
        """
        x: (batch_size, seq_len, d_model)
        use_cache: 是否使用 cache
        """
        batch_size, seq_len, _ = x.size()
        
        # Query（總是計算當前 token）
        Q = self.q_proj(x)
        
        if use_cache and self.k_cache is not None:
            # 只計算新 token 的 K, V
            K_new = self.k_proj(x[:, -1:, :])
            V_new = self.v_proj(x[:, -1:, :])
            
            # 拼接 cache
            K = torch.cat([self.k_cache, K_new], dim=1)
            V = torch.cat([self.v_cache, V_new], dim=1)
            
            # 更新 cache
            self.k_cache = K
            self.v_cache = V
        else:
            # 首次或不使用 cache，計算所有 K, V
            K = self.k_proj(x)
            V = self.v_proj(x)
            
            if use_cache:
                self.k_cache = K
                self.v_cache = V
        
        # 注意力計算
        # 分割成多頭
        Q = Q.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)
        K = K.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)
        V = V.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)
        
        # Scaled dot-product attention
        attn_scores = (Q @ K.transpose(-2, -1)) / (self.head_dim ** 0.5)
        attn_weights = torch.softmax(attn_scores, dim=-1)
        attn_output = attn_weights @ V
        
        # 合併多頭
        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)
        
        # 輸出投影
        output = self.out_proj(attn_output)
        
        return output
    
    def clear_cache(self):
        """清空 cache"""
        self.k_cache = None
        self.v_cache = None

# 測試 KV Cache
kv_attn = KVCacheAttention(d_model=512, num_heads=8)

print("KV Cache")
print("\n沒有 cache:")
print("- 每個 token 都要計算整個序列的 K, V")
print("- 複雜度: O(n²)")
print("\n使用 cache:")
print("- 只計算新 token 的 K, V")
print("- 複雜度: O(n)")
print("- 速度提升: 生成長序列時提升明顯")

# 模擬生成
x = torch.randn(1, 1, 512)  # 第一個 token

# 第一步（建立 cache）
output1 = kv_attn(x, use_cache=True)

# 第二步（使用 cache）
x2 = torch.randn(1, 1, 512)
output2 = kv_attn(x2, use_cache=True)

print(f"\nCache K 形狀: {kv_attn.k_cache.shape}")
print(f"Cache V 形狀: {kv_attn.v_cache.shape}")
</code></pre>

            <h3>Flash Attention</h3>
            <p>Flash Attention 透過重新組織計算，大幅減少 HBM 訪問，加速注意力計算。</p>

            <pre><code class="language-python"># Flash Attention（需要專門的 CUDA 核心）
"""
from flash_attn import flash_attn_func

# 使用 Flash Attention
output = flash_attn_func(
    q, k, v,
    dropout_p=0.0,
    softmax_scale=None,
    causal=True
)
"""

print("\nFlash Attention")
print("創新點:")
print("1. 分塊計算（Tiling）")
print("2. 重新計算（Recomputation）")
print("3. 減少 HBM 訪問")
print("\n效果:")
print("- 速度: 2-4x 提升")
print("- 記憶體: 10-20x 節省")
print("- 適合長序列")
print("\n應用:")
print("- GPT-4, Claude, Llama 2 等都使用")
</code></pre>

            <h3>批次推理優化</h3>
            <pre><code class="language-python">class BatchInferenceOptimizer:
    """
    批次推理優化
    """
    @staticmethod
    def continuous_batching(requests, model, max_batch_size=32):
        """
        連續批次（vLLM 使用的技術）
        
        動態添加和移除請求
        """
        print("連續批次（Continuous Batching）")
        print("優點:")
        print("- 提高吞吐量")
        print("- 減少延遲")
        print("- 更好的 GPU 利用率")
        
        # 簡化實作概念
        active_requests = []
        
        for request in requests:
            active_requests.append(request)
            
            # 達到批次大小或所有請求都在
            if len(active_requests) >= max_batch_size:
                # 批次處理
                batch_results = model(active_requests)
                
                # 移除完成的請求
                active_requests = [
                    req for req, result in zip(active_requests, batch_results)
                    if not result['done']
                ]
    
    @staticmethod
    def speculative_decoding(model, draft_model):
        """
        投機解碼
        
        使用小模型快速生成草稿，大模型驗證
        """
        print("\n投機解碼（Speculative Decoding）")
        print("流程:")
        print("1. 小模型快速生成 k 個 token")
        print("2. 大模型並行驗證這 k 個 token")
        print("3. 接受正確的，拒絕錯誤的")
        print("\n效果:")
        print("- 加速 2-3x")
        print("- 保持大模型質量")

# 展示優化技術
optimizer = BatchInferenceOptimizer()
optimizer.continuous_batching([], None)
optimizer.speculative_decoding(None, None)
</code></pre>

            <div class="tip-box">
                <h4>推理優化總結</h4>
                <ul>
                    <li><strong>KV Cache</strong> - 必備，大幅加速生成</li>
                    <li><strong>Flash Attention</strong> - 長序列必須</li>
                    <li><strong>批次推理</strong> - 提高吞吐量</li>
                    <li><strong>量化</strong> - INT8/INT4 減少記憶體</li>
                    <li><strong>投機解碼</strong> - 2-3x 加速</li>
                </ul>
            </div>
        </div>

        <div class="content">
            <div class="exercise-section">
                <h3>本週練習題</h3>
                
                <h4>基礎練習（必做）</h4>
                <ol>
                    <li>
                        <strong>實作混合精度訓練</strong>
                        <ul>
                            <li>使用 torch.cuda.amp 訓練模型</li>
                            <li>比較 FP32 vs FP16 的速度和記憶體</li>
                            <li>測試梯度縮放的效果</li>
                            <li>分析數值穩定性</li>
                        </ul>
                    </li>
                    
                    <li>
                        <strong>梯度累積實驗</strong>
                        <ul>
                            <li>實現梯度累積訓練</li>
                            <li>比較不同累積步數的效果</li>
                            <li>測試等效大批次效果</li>
                            <li>分析訓練曲線</li>
                        </ul>
                    </li>
                    
                    <li>
                        <strong>模型量化</strong>
                        <ul>
                            <li>動態量化 vs 靜態量化</li>
                            <li>測量模型大小和速度</li>
                            <li>評估精度損失</li>
                            <li>在 CPU 上測試推理速度</li>
                        </ul>
                    </li>
                </ol>

                <h4>進階練習</h4>
                <ol start="4">
                    <li>
                        <strong>實作 KV Cache</strong>
                        <ul>
                            <li>在 Transformer 中實現 KV Cache</li>
                            <li>測試生成速度提升</li>
                            <li>分析記憶體使用</li>
                            <li>處理不同序列長度</li>
                        </ul>
                    </li>
                    
                    <li>
                        <strong>分散式訓練</strong>
                        <ul>
                            <li>使用 DistributedDataParallel</li>
                            <li>多 GPU 訓練實驗</li>
                            <li>測量擴展效率</li>
                            <li>優化通訊開銷</li>
                        </ul>
                    </li>
                </ol>

                <h4>挑戰練習</h4>
                <ol start="6">
                    <li>
                        <strong>完整優化 Pipeline</strong>
                        <ul>
                            <li>結合混合精度、梯度累積、檢查點</li>
                            <li>訓練大型模型（如 GPT-2）</li>
                            <li>比較不同優化組合</li>
                            <li>記錄訓練效率指標</li>
                        </ul>
                    </li>
                    
                    <li>
                        <strong>INT4 量化實驗</strong>
                        <ul>
                            <li>使用 GPTQ 或 AWQ</li>
                            <li>量化 Llama 或 GPT 模型</li>
                            <li>評估生成質量</li>
                            <li>測量推理性能</li>
                        </ul>
                    </li>
                </ol>
            </div>
        </div>

        <div class="content">
            <h2>本週總結</h2>
            
            <h3>重點回顧</h3>
            <ul>
                <li>理解分散式訓練的三種並行策略</li>
                <li>掌握混合精度訓練技術</li>
                <li>學會梯度累積和檢查點</li>
                <li>理解模型量化原理與實作</li>
                <li>掌握推理優化技術</li>
            </ul>

            <h3>關鍵概念</h3>
            <ul>
                <li><strong>並行策略</strong> - 資料、模型、流水線並行</li>
                <li><strong>混合精度</strong> - FP16/BF16 加速訓練</li>
                <li><strong>記憶體優化</strong> - 累積、檢查點、ZeRO</li>
                <li><strong>量化</strong> - INT8/INT4 壓縮模型</li>
                <li><strong>推理加速</strong> - KV Cache、Flash Attention</li>
            </ul>

            <h3>訓練大型模型的最佳實踐</h3>
            <table>
                <tr>
                    <th>模型規模</th>
                    <th>推薦策略</th>
                    <th>硬體需求</th>
                </tr>
                <tr>
                    <td>< 1B</td>
                    <td>Data Parallel + FP16</td>
                    <td>1-4 GPU</td>
                </tr>
                <tr>
                    <td>1B - 10B</td>
                    <td>FSDP + BF16 + Gradient Checkpointing</td>
                    <td>4-16 GPU</td>
                </tr>
                <tr>
                    <td>10B - 100B</td>
                    <td>混合並行 + ZeRO + Flash Attention</td>
                    <td>16-128 GPU</td>
                </tr>
                <tr>
                    <td>> 100B</td>
                    <td>3D Parallel + Pipeline + 全部優化</td>
                    <td>128+ GPU</td>
                </tr>
            </table>

            <h3>下週預告</h3>
            <div class="note-box">
                <h4>Week 14 - 評估與基準測試</h4>
                <p>下週將學習如何全面評估 LLM：</p>
                <ul>
                    <li>自動評估指標（BLEU、ROUGE、BERTScore）</li>
                    <li>人工評估方法</li>
                    <li>主流基準測試（MMLU、HumanEval、TruthfulQA）</li>
                    <li>安全性與偏見評估</li>
                    <li>實戰：構建評估框架</li>
                </ul>
            </div>
        </div>

        <div class="nav-buttons">
            <a href="llm-week12.html" class="btn btn-secondary">Week 12：RLHF 與對齊</a>
            <a href="llm-week14.html" class="btn">Week 14：評估與基準</a>
        </div>

        <footer>
            <p>LLM 與 AI/ML 課程 Week 13</p>
            <p>分散式訓練 · 混合精度 · 梯度累積 · 模型量化 · 推理優化 · KV Cache · Flash Attention</p>
            <p style="font-size: 0.9em; color: #95a5a6;">© 2025 All Rights Reserved</p>
        </footer>
    </div>
</body>
</html>