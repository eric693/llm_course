<!DOCTYPE html>
<html lang="zh-TW">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Week 5 - 卷積神經網路（CNN） | LLM 與 AI/ML 課程</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', 'Microsoft JhengHei', Arial, sans-serif;
            background: #f8f9fa;
            color: #1a1a1a;
            line-height: 1.7;
        }

        .container {
            max-width: 1000px;
            margin: 0 auto;
            padding: 20px;
        }

        .nav {
            background: white;
            padding: 16px 28px;
            border-radius: 8px;
            margin-bottom: 32px;
            display: flex;
            justify-content: space-between;
            align-items: center;
            box-shadow: 0 2px 8px rgba(0,0,0,0.08);
            border: 1px solid #e0e0e0;
        }

        .nav a {
            color: #3498db;
            text-decoration: none;
            font-weight: 500;
            padding: 8px 16px;
            border-radius: 4px;
            transition: all 0.2s ease;
        }

        .nav a:hover {
            background: #ecf0f1;
            color: #2980b9;
        }

        .nav span {
            color: #6c757d;
            font-weight: 500;
        }

        header {
            background: linear-gradient(to right, #2c3e50, #34495e);
            color: white;
            padding: 56px 40px;
            border-radius: 8px;
            margin-bottom: 32px;
            box-shadow: 0 4px 12px rgba(0,0,0,0.15);
            border-bottom: 3px solid #3498db;
        }

        header h1 {
            font-size: 2.2em;
            margin-bottom: 12px;
            font-weight: 700;
        }

        header .subtitle {
            font-size: 1.05em;
            opacity: 0.9;
            font-weight: 400;
        }

        .content {
            background: white;
            border-radius: 8px;
            padding: 48px;
            margin-bottom: 32px;
            box-shadow: 0 2px 8px rgba(0,0,0,0.08);
            border: 1px solid #e0e0e0;
        }

        .content h2 {
            color: #1a1a1a;
            font-size: 1.9em;
            margin-bottom: 24px;
            padding-bottom: 12px;
            border-bottom: 2px solid #3498db;
            font-weight: 700;
        }

        .content h3 {
            color: #2c3e50;
            font-size: 1.5em;
            margin: 36px 0 16px 0;
            font-weight: 600;
        }

        .content h4 {
            color: #34495e;
            font-size: 1.2em;
            margin: 24px 0 12px 0;
            font-weight: 600;
        }

        .content p {
            margin-bottom: 16px;
            line-height: 1.8;
            color: #4a4a4a;
        }

        pre[class*="language-"] {
            margin: 24px 0;
            border-radius: 6px;
            border: 1px solid #d0d7de;
            box-shadow: 0 1px 3px rgba(0,0,0,0.08);
        }

        code[class*="language-"] {
            font-family: 'SF Mono', Monaco, 'Cascadia Code', 'Roboto Mono', Consolas, 'Courier New', monospace;
            font-size: 0.92em;
            line-height: 1.6;
        }

        .note-box {
            background: #e3f2fd;
            border-left: 4px solid #2196f3;
            padding: 20px 24px;
            margin: 24px 0;
            border-radius: 4px;
        }

        .note-box h4 {
            color: #1565c0;
            margin-top: 0;
            margin-bottom: 12px;
            font-size: 1.1em;
        }

        .warning-box {
            background: #fff3e0;
            border-left: 4px solid #ff9800;
            padding: 20px 24px;
            margin: 24px 0;
            border-radius: 4px;
        }

        .warning-box h4 {
            color: #e65100;
            margin-top: 0;
            margin-bottom: 12px;
            font-size: 1.1em;
        }

        .tip-box {
            background: #f1f8e9;
            border-left: 4px solid #8bc34a;
            padding: 20px 24px;
            margin: 24px 0;
            border-radius: 4px;
        }

        .tip-box h4 {
            color: #558b2f;
            margin-top: 0;
            margin-bottom: 12px;
            font-size: 1.1em;
        }

        .concept-box {
            background: #f3e5f5;
            border-left: 4px solid #9c27b0;
            padding: 20px 24px;
            margin: 24px 0;
            border-radius: 4px;
        }

        .concept-box h4 {
            color: #6a1b9a;
            margin-top: 0;
            margin-bottom: 12px;
            font-size: 1.1em;
        }

        .exercise-section {
            background: #fafafa;
            border: 1px solid #e0e0e0;
            padding: 32px;
            margin: 32px 0;
            border-radius: 8px;
        }

        .exercise-section h3 {
            color: #2c3e50;
            margin-top: 0;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin: 24px 0;
            background: white;
        }

        table th,
        table td {
            padding: 14px 16px;
            text-align: left;
            border: 1px solid #dee2e6;
        }

        table th {
            background: #2c3e50;
            color: white;
            font-weight: 600;
        }

        table tr:nth-child(even) {
            background: #f8f9fa;
        }

        ul, ol {
            margin-left: 28px;
            margin-top: 12px;
            margin-bottom: 16px;
        }

        li {
            margin: 10px 0;
            line-height: 1.7;
            color: #4a4a4a;
        }

        .nav-buttons {
            display: flex;
            justify-content: space-between;
            margin-top: 48px;
            gap: 16px;
        }

        .btn {
            display: inline-block;
            padding: 12px 28px;
            background: #3498db;
            color: white;
            text-decoration: none;
            border-radius: 6px;
            font-weight: 500;
            transition: all 0.2s ease;
            border: none;
        }

        .btn:hover {
            background: #2980b9;
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(52, 152, 219, 0.3);
        }

        .btn-secondary {
            background: #95a5a6;
        }

        .btn-secondary:hover {
            background: #7f8c8d;
            box-shadow: 0 4px 12px rgba(149, 165, 166, 0.3);
        }

        .highlight {
            background: #fff9c4;
            padding: 2px 6px;
            border-radius: 3px;
            font-weight: 500;
        }

        footer {
            text-align: center;
            padding: 32px 0;
            color: #6c757d;
            margin-top: 48px;
            border-top: 1px solid #dee2e6;
        }

        footer p {
            margin: 8px 0;
        }

        @media (max-width: 768px) {
            .container {
                padding: 12px;
            }

            .content {
                padding: 24px 20px;
            }

            header {
                padding: 32px 24px;
            }

            header h1 {
                font-size: 1.8em;
            }

            .nav-buttons {
                flex-direction: column;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <div class="nav">
            <a href="llm-week4.html">Week 4</a>
            <span>Week 5 / 16</span>
            <a href="llm-week6.html">Week 6</a>
        </div>

        <header>
            <h1>Week 5 - 卷積神經網路（CNN）</h1>
            <div class="subtitle">掌握電腦視覺的核心技術與經典 CNN 架構</div>
        </header>

        <div class="content">
            <h2>本週學習內容</h2>
            <ul>
                <li>卷積層與池化層原理</li>
                <li>經典 CNN 架構（LeNet、VGG、ResNet）</li>
                <li>遷移學習與預訓練模型</li>
                <li>資料增強技術</li>
                <li>實戰：CIFAR-10 影像分類</li>
            </ul>

            <div class="concept-box">
                <h4>為什麼要使用 CNN？</h4>
                <p>相比全連接網路，CNN 在影像處理上有顯著優勢：</p>
                <ul>
                    <li><strong>參數共享</strong> - 同一個卷積核在整張圖上使用，大幅減少參數</li>
                    <li><strong>局部連接</strong> - 每個神經元只關注局部區域，符合視覺特性</li>
                    <li><strong>平移不變性</strong> - 物體在圖中位置改變，也能辨識</li>
                    <li><strong>層次化特徵</strong> - 淺層學邊緣，深層學複雜模式</li>
                </ul>
            </div>
        </div>

        <div class="content">
            <h2>1. 卷積層（Convolutional Layer）</h2>
            
            <h3>卷積運算原理</h3>
            <p><span class="highlight">卷積</span>是一種特殊的線性運算，使用卷積核（filter/kernel）在輸入上滑動，計算局部區域的加權和。</p>

            <pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt

def convolution_2d(image, kernel, stride=1, padding=0):
    """
    2D 卷積運算
    
    參數:
        image: 輸入影像 (H, W)
        kernel: 卷積核 (K, K)
        stride: 步長
        padding: 填充
    """
    # 添加 padding
    if padding > 0:
        image = np.pad(image, padding, mode='constant')
    
    H, W = image.shape
    K = kernel.shape[0]
    
    # 計算輸出大小
    out_H = (H - K) // stride + 1
    out_W = (W - K) // stride + 1
    
    output = np.zeros((out_H, out_W))
    
    # 卷積運算
    for i in range(out_H):
        for j in range(out_W):
            h_start = i * stride
            w_start = j * stride
            
            # 提取區域
            region = image[h_start:h_start+K, w_start:w_start+K]
            
            # 元素相乘後求和
            output[i, j] = np.sum(region * kernel)
    
    return output

# 建立測試影像
image = np.array([
    [1, 2, 3, 4, 5],
    [6, 7, 8, 9, 10],
    [11, 12, 13, 14, 15],
    [16, 17, 18, 19, 20],
    [21, 22, 23, 24, 25]
])

# 不同的卷積核
kernels = {
    '垂直邊緣': np.array([[-1, 0, 1],
                         [-1, 0, 1],
                         [-1, 0, 1]]),
    
    '水平邊緣': np.array([[-1, -1, -1],
                         [ 0,  0,  0],
                         [ 1,  1,  1]]),
    
    '銳化': np.array([[ 0, -1,  0],
                     [-1,  5, -1],
                     [ 0, -1,  0]]),
    
    '模糊': np.array([[1, 1, 1],
                     [1, 1, 1],
                     [1, 1, 1]]) / 9
}

# 視覺化卷積效果
fig, axes = plt.subplots(2, 3, figsize=(15, 10))

axes[0, 0].imshow(image, cmap='gray')
axes[0, 0].set_title('原始影像')
axes[0, 0].axis('off')

for idx, (name, kernel) in enumerate(kernels.items()):
    result = convolution_2d(image, kernel, stride=1, padding=0)
    
    row = (idx + 1) // 3
    col = (idx + 1) % 3
    
    axes[row, col].imshow(result, cmap='gray')
    axes[row, col].set_title(f'{name}卷積核')
    axes[row, col].axis('off')

plt.tight_layout()
plt.show()

print("卷積參數說明:")
print(f"輸入大小: {image.shape}")
print(f"卷積核大小: 3x3")
print(f"步長: 1")
print(f"輸出大小: {result.shape}")
</code></pre>

            <div class="note-box">
                <h4>卷積運算的關鍵參數</h4>
                <ul>
                    <li><strong>卷積核大小（Kernel Size）</strong> - 常見：3×3、5×5、7×7</li>
                    <li><strong>步長（Stride）</strong> - 卷積核移動的步幅，影響輸出大小</li>
                    <li><strong>填充（Padding）</strong> - 邊緣填充，保持輸出大小</li>
                    <li><strong>通道數（Channels）</strong> - 卷積核數量，決定輸出通道數</li>
                </ul>
            </div>

            <h3>輸出大小計算</h3>
            <pre><code class="language-python">def calculate_output_size(input_size, kernel_size, stride, padding):
    """
    計算卷積層輸出大小
    
    公式: output_size = (input_size - kernel_size + 2*padding) / stride + 1
    """
    output_size = (input_size - kernel_size + 2 * padding) // stride + 1
    return output_size

# 範例
print("不同參數組合的輸出大小:")
print(f"輸入: 28×28, 核: 3×3, 步長: 1, 填充: 0 → {calculate_output_size(28, 3, 1, 0)}×{calculate_output_size(28, 3, 1, 0)}")
print(f"輸入: 28×28, 核: 3×3, 步長: 1, 填充: 1 → {calculate_output_size(28, 3, 1, 1)}×{calculate_output_size(28, 3, 1, 1)}")
print(f"輸入: 28×28, 核: 5×5, 步長: 2, 填充: 0 → {calculate_output_size(28, 5, 2, 0)}×{calculate_output_size(28, 5, 2, 0)}")
print(f"輸入: 224×224, 核: 7×7, 步長: 2, 填充: 3 → {calculate_output_size(224, 7, 2, 3)}×{calculate_output_size(224, 7, 2, 3)}")
</code></pre>

            <h3>在真實影像上測試</h3>
            <pre><code class="language-python">from PIL import Image
import requests
from io import BytesIO

# 載入測試影像（或使用本地影像）
# 這裡使用簡單的測試圖案
def create_test_image():
    img = np.zeros((100, 100))
    # 繪製矩形
    img[20:40, 20:80] = 1
    img[60:80, 20:80] = 1
    # 繪製圓形
    y, x = np.ogrid[-50:50, -50:50]
    mask = x*x + y*y <= 20*20
    img[mask] = 1
    return img

test_image = create_test_image()

# 邊緣檢測核
edge_kernel = np.array([[-1, -1, -1],
                        [-1,  8, -1],
                        [-1, -1, -1]])

# 應用卷積
edges = convolution_2d(test_image, edge_kernel, padding=1)

# 視覺化
fig, axes = plt.subplots(1, 2, figsize=(12, 5))

axes[0].imshow(test_image, cmap='gray')
axes[0].set_title('原始影像')
axes[0].axis('off')

axes[1].imshow(edges, cmap='gray')
axes[1].set_title('邊緣檢測結果')
axes[1].axis('off')

plt.tight_layout()
plt.show()
</code></pre>
        </div>

        <div class="content">
            <h2>2. 池化層（Pooling Layer）</h2>
            
            <h3>池化的目的</h3>
            <p><span class="highlight">池化</span>用於降低特徵圖的空間維度，減少參數量和計算量，同時提供平移不變性。</p>

            <pre><code class="language-python">import numpy as np

def max_pooling_2d(image, pool_size=2, stride=2):
    """最大池化"""
    H, W = image.shape
    out_H = (H - pool_size) // stride + 1
    out_W = (W - pool_size) // stride + 1
    
    output = np.zeros((out_H, out_W))
    
    for i in range(out_H):
        for j in range(out_W):
            h_start = i * stride
            w_start = j * stride
            
            # 提取區域並取最大值
            region = image[h_start:h_start+pool_size, 
                          w_start:w_start+pool_size]
            output[i, j] = np.max(region)
    
    return output

def average_pooling_2d(image, pool_size=2, stride=2):
    """平均池化"""
    H, W = image.shape
    out_H = (H - pool_size) // stride + 1
    out_W = (W - pool_size) // stride + 1
    
    output = np.zeros((out_H, out_W))
    
    for i in range(out_H):
        for j in range(out_W):
            h_start = i * stride
            w_start = j * stride
            
            # 提取區域並計算平均值
            region = image[h_start:h_start+pool_size, 
                          w_start:w_start+pool_size]
            output[i, j] = np.mean(region)
    
    return output

# 測試
test_image = np.random.randint(0, 10, size=(8, 8))

max_pooled = max_pooling_2d(test_image, pool_size=2, stride=2)
avg_pooled = average_pooling_2d(test_image, pool_size=2, stride=2)

print("原始影像 (8×8):")
print(test_image)
print(f"\n最大池化後 (4×4):")
print(max_pooled)
print(f"\n平均池化後 (4×4):")
print(avg_pooled)

# 視覺化
fig, axes = plt.subplots(1, 3, figsize=(15, 5))

axes[0].imshow(test_image, cmap='viridis')
axes[0].set_title('原始影像 (8×8)')
axes[0].axis('off')

axes[1].imshow(max_pooled, cmap='viridis')
axes[1].set_title('最大池化 (4×4)')
axes[1].axis('off')

axes[2].imshow(avg_pooled, cmap='viridis')
axes[2].set_title('平均池化 (4×4)')
axes[2].axis('off')

plt.tight_layout()
plt.show()
</code></pre>

            <div class="tip-box">
                <h4>池化層的選擇</h4>
                <ul>
                    <li><strong>最大池化（Max Pooling）</strong> - 最常用，保留最顯著特徵</li>
                    <li><strong>平均池化（Average Pooling）</strong> - 保留整體資訊，較平滑</li>
                    <li><strong>全局平均池化（Global Average Pooling）</strong> - 替代全連接層，減少參數</li>
                    <li><strong>常見設定</strong> - 2×2 窗口，步長為 2</li>
                </ul>
            </div>
        </div>

        <div class="content">
            <h2>3. 使用 PyTorch 建立 CNN</h2>
            
            <h3>簡單的 CNN 架構</h3>
            <pre><code class="language-python">import torch
import torch.nn as nn
import torch.nn.functional as F

class SimpleCNN(nn.Module):
    def __init__(self, num_classes=10):
        super(SimpleCNN, self).__init__()
        
        # 卷積層 1
        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, 
                               kernel_size=3, stride=1, padding=1)
        self.bn1 = nn.BatchNorm2d(32)
        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)
        
        # 卷積層 2
        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, 
                               kernel_size=3, stride=1, padding=1)
        self.bn2 = nn.BatchNorm2d(64)
        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)
        
        # 卷積層 3
        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, 
                               kernel_size=3, stride=1, padding=1)
        self.bn3 = nn.BatchNorm2d(128)
        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)
        
        # 全連接層
        self.fc1 = nn.Linear(128 * 3 * 3, 256)
        self.dropout = nn.Dropout(0.5)
        self.fc2 = nn.Linear(256, num_classes)
    
    def forward(self, x):
        # 卷積塊 1
        x = self.conv1(x)
        x = self.bn1(x)
        x = F.relu(x)
        x = self.pool1(x)
        
        # 卷積塊 2
        x = self.conv2(x)
        x = self.bn2(x)
        x = F.relu(x)
        x = self.pool2(x)
        
        # 卷積塊 3
        x = self.conv3(x)
        x = self.bn3(x)
        x = F.relu(x)
        x = self.pool3(x)
        
        # 展平
        x = x.view(x.size(0), -1)
        
        # 全連接層
        x = self.fc1(x)
        x = F.relu(x)
        x = self.dropout(x)
        x = self.fc2(x)
        
        return x

# 建立模型
model = SimpleCNN(num_classes=10)
print(model)

# 測試前向傳播
x = torch.randn(4, 1, 28, 28)  # batch_size=4, channels=1, H=28, W=28
output = model(x)
print(f"\n輸入形狀: {x.shape}")
print(f"輸出形狀: {output.shape}")

# 計算參數量
total_params = sum(p.numel() for p in model.parameters())
trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
print(f"\n總參數量: {total_params:,}")
print(f"可訓練參數: {trainable_params:,}")
</code></pre>

            <h3>追蹤特徵圖尺寸</h3>
            <pre><code class="language-python">class CNNWithPrint(nn.Module):
    """帶有形狀追蹤的 CNN"""
    def __init__(self):
        super(CNNWithPrint, self).__init__()
        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)
        self.pool1 = nn.MaxPool2d(2, 2)
        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)
        self.pool2 = nn.MaxPool2d(2, 2)
    
    def forward(self, x):
        print(f"輸入: {x.shape}")
        
        x = F.relu(self.conv1(x))
        print(f"Conv1 後: {x.shape}")
        
        x = self.pool1(x)
        print(f"Pool1 後: {x.shape}")
        
        x = F.relu(self.conv2(x))
        print(f"Conv2 後: {x.shape}")
        
        x = self.pool2(x)
        print(f"Pool2 後: {x.shape}")
        
        return x

# 測試
model = CNNWithPrint()
x = torch.randn(1, 3, 32, 32)  # CIFAR-10 大小
output = model(x)
</code></pre>
        </div>

        <div class="content">
            <h2>4. 經典 CNN 架構</h2>
            
            <h3>LeNet-5（1998）</h3>
            <p>最早的 CNN 架構之一，用於手寫數字辨識。</p>

            <pre><code class="language-python">import torch.nn as nn

class LeNet5(nn.Module):
    def __init__(self, num_classes=10):
        super(LeNet5, self).__init__()
        
        self.conv1 = nn.Conv2d(1, 6, kernel_size=5)
        self.pool1 = nn.AvgPool2d(kernel_size=2, stride=2)
        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)
        self.pool2 = nn.AvgPool2d(kernel_size=2, stride=2)
        
        self.fc1 = nn.Linear(16 * 4 * 4, 120)
        self.fc2 = nn.Linear(120, 84)
        self.fc3 = nn.Linear(84, num_classes)
    
    def forward(self, x):
        x = self.pool1(F.relu(self.conv1(x)))
        x = self.pool2(F.relu(self.conv2(x)))
        x = x.view(x.size(0), -1)
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return x

model = LeNet5()
print("LeNet-5 架構:")
print(model)
print(f"\n參數量: {sum(p.numel() for p in model.parameters()):,}")
</code></pre>

            <h3>VGG（2014）</h3>
            <p>使用小的 3×3 卷積核堆疊，簡單但有效。</p>

            <pre><code class="language-python">class VGG16(nn.Module):
    def __init__(self, num_classes=1000):
        super(VGG16, self).__init__()
        
        self.features = nn.Sequential(
            # Block 1
            nn.Conv2d(3, 64, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 64, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            
            # Block 2
            nn.Conv2d(64, 128, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(128, 128, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            
            # Block 3
            nn.Conv2d(128, 256, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            
            # Block 4
            nn.Conv2d(256, 512, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(512, 512, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(512, 512, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            
            # Block 5
            nn.Conv2d(512, 512, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(512, 512, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(512, 512, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
        )
        
        self.avgpool = nn.AdaptiveAvgPool2d((7, 7))
        
        self.classifier = nn.Sequential(
            nn.Linear(512 * 7 * 7, 4096),
            nn.ReLU(inplace=True),
            nn.Dropout(0.5),
            nn.Linear(4096, 4096),
            nn.ReLU(inplace=True),
            nn.Dropout(0.5),
            nn.Linear(4096, num_classes),
        )
    
    def forward(self, x):
        x = self.features(x)
        x = self.avgpool(x)
        x = torch.flatten(x, 1)
        x = self.classifier(x)
        return x

# 建立簡化版 VGG
model = VGG16(num_classes=10)
print("VGG16 架構（簡化版）")
print(f"參數量: {sum(p.numel() for p in model.parameters()):,}")
</code></pre>

            <h3>ResNet（2015）</h3>
            <p>引入殘差連接（skip connection），解決深層網路的梯度消失問題。</p>

            <pre><code class="language-python">class ResidualBlock(nn.Module):
    """殘差塊"""
    def __init__(self, in_channels, out_channels, stride=1):
        super(ResidualBlock, self).__init__()
        
        self.conv1 = nn.Conv2d(in_channels, out_channels, 
                               kernel_size=3, stride=stride, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(out_channels)
        
        self.conv2 = nn.Conv2d(out_channels, out_channels, 
                               kernel_size=3, stride=1, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(out_channels)
        
        # 捷徑連接
        self.shortcut = nn.Sequential()
        if stride != 1 or in_channels != out_channels:
            self.shortcut = nn.Sequential(
                nn.Conv2d(in_channels, out_channels, 
                         kernel_size=1, stride=stride, bias=False),
                nn.BatchNorm2d(out_channels)
            )
    
    def forward(self, x):
        identity = x
        
        out = F.relu(self.bn1(self.conv1(x)))
        out = self.bn2(self.conv2(out))
        
        # 殘差連接
        out += self.shortcut(identity)
        out = F.relu(out)
        
        return out

class ResNet(nn.Module):
    def __init__(self, num_blocks, num_classes=10):
        super(ResNet, self).__init__()
        
        self.in_channels = 64
        
        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(64)
        
        self.layer1 = self._make_layer(64, num_blocks[0], stride=1)
        self.layer2 = self._make_layer(128, num_blocks[1], stride=2)
        self.layer3 = self._make_layer(256, num_blocks[2], stride=2)
        self.layer4 = self._make_layer(512, num_blocks[3], stride=2)
        
        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
        self.fc = nn.Linear(512, num_classes)
    
    def _make_layer(self, out_channels, num_blocks, stride):
        strides = [stride] + [1] * (num_blocks - 1)
        layers = []
        
        for stride in strides:
            layers.append(ResidualBlock(self.in_channels, out_channels, stride))
            self.in_channels = out_channels
        
        return nn.Sequential(*layers)
    
    def forward(self, x):
        out = F.relu(self.bn1(self.conv1(x)))
        out = self.layer1(out)
        out = self.layer2(out)
        out = self.layer3(out)
        out = self.layer4(out)
        out = self.avgpool(out)
        out = torch.flatten(out, 1)
        out = self.fc(out)
        return out

# ResNet-18
model = ResNet([2, 2, 2, 2], num_classes=10)
print("ResNet-18 架構")
print(f"參數量: {sum(p.numel() for p in model.parameters()):,}")

# 測試前向傳播
x = torch.randn(1, 3, 32, 32)
output = model(x)
print(f"\n輸入: {x.shape}")
print(f"輸出: {output.shape}")
</code></pre>

            <div class="concept-box">
                <h4>CNN 架構演進</h4>
                <table>
                    <tr>
                        <th>架構</th>
                        <th>年份</th>
                        <th>創新點</th>
                        <th>參數量</th>
                    </tr>
                    <tr>
                        <td>LeNet-5</td>
                        <td>1998</td>
                        <td>首個成功的 CNN</td>
                        <td>~60K</td>
                    </tr>
                    <tr>
                        <td>AlexNet</td>
                        <td>2012</td>
                        <td>ReLU、Dropout、GPU 訓練</td>
                        <td>~60M</td>
                    </tr>
                    <tr>
                        <td>VGG</td>
                        <td>2014</td>
                        <td>小卷積核堆疊</td>
                        <td>~138M</td>
                    </tr>
                    <tr>
                        <td>GoogLeNet</td>
                        <td>2014</td>
                        <td>Inception 模組</td>
                        <td>~7M</td>
                    </tr>
                    <tr>
                        <td>ResNet</td>
                        <td>2015</td>
                        <td>殘差連接、極深網路</td>
                        <td>~25M</td>
                    </tr>
                    <tr>
                        <td>EfficientNet</td>
                        <td>2019</td>
                        <td>複合縮放、高效率</td>
                        <td>~5M</td>
                    </tr>
                </table>
            </div>
        </div>

        <div class="content">
            <h2>5. 遷移學習（Transfer Learning）</h2>
            
            <h3>什麼是遷移學習？</h3>
            <p><span class="highlight">遷移學習</span>是利用在大型資料集（如 ImageNet）上預訓練的模型，應用到新任務上。</p>

            <div class="note-box">
                <h4>為什麼使用遷移學習？</h4>
                <ul>
                    <li><strong>節省時間</strong> - 不需從頭訓練，幾小時內完成</li>
                    <li><strong>小資料集</strong> - 即使資料少也能獲得好結果</li>
                    <li><strong>特徵重用</strong> - 預訓練模型已學會通用特徵（邊緣、紋理等）</li>
                    <li><strong>避免過擬合</strong> - 預訓練權重提供好的初始化</li>
                </ul>
            </div>

            <h3>使用預訓練模型</h3>
            <pre><code class="language-python">import torch
import torch.nn as nn
import torchvision.models as models

# 方法 1: 固定特徵提取
def create_feature_extractor(num_classes):
    # 載入預訓練的 ResNet18
    model = models.resnet18(pretrained=True)
    
    # 凍結所有層
    for param in model.parameters():
        param.requires_grad = False
    
    # 替換最後的全連接層
    num_features = model.fc.in_features
    model.fc = nn.Linear(num_features, num_classes)
    
    return model

# 方法 2: 微調（Fine-tuning）
def create_finetuned_model(num_classes):
    # 載入預訓練模型
    model = models.resnet18(pretrained=True)
    
    # 凍結前面的層
    for name, param in model.named_parameters():
        if "layer4" not in name and "fc" not in name:
            param.requires_grad = False
    
    # 替換分類器
    num_features = model.fc.in_features
    model.fc = nn.Linear(num_features, num_classes)
    
    return model

# 建立模型
feature_extractor = create_feature_extractor(num_classes=10)
finetuned_model = create_finetuned_model(num_classes=10)

print("特徵提取模型:")
print(f"可訓練參數: {sum(p.numel() for p in feature_extractor.parameters() if p.requires_grad):,}")
print(f"總參數: {sum(p.numel() for p in feature_extractor.parameters()):,}")

print("\n微調模型:")
print(f"可訓練參數: {sum(p.numel() for p in finetuned_model.parameters() if p.requires_grad):,}")
print(f"總參數: {sum(p.numel() for p in finetuned_model.parameters()):,}")
</code></pre>

            <h3>遷移學習訓練策略</h3>
            <pre><code class="language-python">import torch.optim as optim

def train_with_transfer_learning(model, train_loader, val_loader, 
                                 num_epochs=10, device='cuda'):
    """
    遷移學習訓練策略
    """
    criterion = nn.CrossEntropyLoss()
    
    # 不同層使用不同學習率
    params = [
        {'params': model.fc.parameters(), 'lr': 1e-3},  # 新層使用較大學習率
        {'params': [p for n, p in model.named_parameters() 
                   if 'fc' not in n and p.requires_grad], 'lr': 1e-4}  # 預訓練層使用小學習率
    ]
    
    optimizer = optim.Adam(params)
    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)
    
    model = model.to(device)
    best_acc = 0.0
    
    for epoch in range(num_epochs):
        # 訓練階段
        model.train()
        train_loss = 0.0
        train_correct = 0
        train_total = 0
        
        for inputs, labels in train_loader:
            inputs, labels = inputs.to(device), labels.to(device)
            
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            
            train_loss += loss.item()
            _, predicted = outputs.max(1)
            train_total += labels.size(0)
            train_correct += predicted.eq(labels).sum().item()
        
        # 驗證階段
        model.eval()
        val_loss = 0.0
        val_correct = 0
        val_total = 0
        
        with torch.no_grad():
            for inputs, labels in val_loader:
                inputs, labels = inputs.to(device), labels.to(device)
                outputs = model(inputs)
                loss = criterion(outputs, labels)
                
                val_loss += loss.item()
                _, predicted = outputs.max(1)
                val_total += labels.size(0)
                val_correct += predicted.eq(labels).sum().item()
        
        train_acc = 100. * train_correct / train_total
        val_acc = 100. * val_correct / val_total
        
        print(f'Epoch {epoch+1}/{num_epochs}:')
        print(f'  Train Loss: {train_loss/len(train_loader):.4f}, Acc: {train_acc:.2f}%')
        print(f'  Val Loss: {val_loss/len(val_loader):.4f}, Acc: {val_acc:.2f}%')
        
        # 儲存最佳模型
        if val_acc > best_acc:
            best_acc = val_acc
            torch.save(model.state_dict(), 'best_model.pth')
        
        scheduler.step()
    
    print(f'\nBest Validation Accuracy: {best_acc:.2f}%')
    return model

# 使用範例
# model = create_finetuned_model(num_classes=10)
# model = train_with_transfer_learning(model, train_loader, val_loader)
</code></pre>

            <div class="tip-box">
                <h4>遷移學習最佳實踐</h4>
                <ul>
                    <li><strong>資料相似度高</strong> - 微調更多層</li>
                    <li><strong>資料量小</strong> - 只訓練最後幾層</li>
                    <li><strong>學習率設定</strong> - 新層用大學習率，預訓練層用小學習率</li>
                    <li><strong>逐步解凍</strong> - 先訓練分類器，再逐漸解凍更多層</li>
                </ul>
            </div>
        </div>

        <div class="content">
            <h2>6. 資料增強（Data Augmentation）</h2>
            
            <h3>為什麼需要資料增強？</h3>
            <p>資料增強透過對原始資料進行變換，人工擴充訓練集，防止過擬合並提升模型泛化能力。</p>

            <pre><code class="language-python">import torchvision.transforms as transforms
from PIL import Image
import matplotlib.pyplot as plt

# 定義各種資料增強
transform_list = {
    '原始': transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
    ]),
    
    '水平翻轉': transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.RandomHorizontalFlip(p=1.0),
        transforms.ToTensor(),
    ]),
    
    '隨機裁切': transforms.Compose([
        transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),
        transforms.ToTensor(),
    ]),
    
    '顏色調整': transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ColorJitter(brightness=0.5, contrast=0.5, 
                              saturation=0.5, hue=0.2),
        transforms.ToTensor(),
    ]),
    
    '旋轉': transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.RandomRotation(30),
        transforms.ToTensor(),
    ]),
    
    '組合': transforms.Compose([
        transforms.RandomResizedCrop(224),
        transforms.RandomHorizontalFlip(),
        transforms.ColorJitter(brightness=0.2, contrast=0.2),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406],
                           std=[0.229, 0.224, 0.225])
    ])
}

# 載入測試影像
# image = Image.open('test_image.jpg')

# 或建立測試影像
import numpy as np
test_array = np.random.randint(0, 255, (224, 224, 3), dtype=np.uint8)
image = Image.fromarray(test_array)

# 視覺化不同的資料增強效果
fig, axes = plt.subplots(2, 3, figsize=(15, 10))
axes = axes.ravel()

for idx, (name, transform) in enumerate(transform_list.items()):
    if idx >= 6:
        break
    
    augmented = transform(image)
    
    if name == '組合':
        # 反標準化以便視覺化
        mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)
        std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)
        augmented = augmented * std + mean
        augmented = torch.clamp(augmented, 0, 1)
    
    axes[idx].imshow(augmented.permute(1, 2, 0))
    axes[idx].set_title(name)
    axes[idx].axis('off')

plt.tight_layout()
plt.show()
</code></pre>

            <h3>常用的資料增強策略</h3>
            <pre><code class="language-python"># 訓練時的資料增強
train_transform = transforms.Compose([
    # 隨機裁切並調整大小
    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),
    
    # 隨機水平翻轉
    transforms.RandomHorizontalFlip(p=0.5),
    
    # 隨機旋轉
    transforms.RandomRotation(15),
    
    # 顏色抖動
    transforms.ColorJitter(
        brightness=0.2,  # 亮度
        contrast=0.2,    # 對比度
        saturation=0.2,  # 飽和度
        hue=0.1          # 色調
    ),
    
    # 隨機透視變換
    transforms.RandomPerspective(distortion_scale=0.2, p=0.5),
    
    # 轉換為張量
    transforms.ToTensor(),
    
    # 標準化（使用 ImageNet 的均值和標準差）
    transforms.Normalize(
        mean=[0.485, 0.456, 0.406],
        std=[0.229, 0.224, 0.225]
    )
])

# 測試時不使用資料增強（只做基本預處理）
test_transform = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
    transforms.Normalize(
        mean=[0.485, 0.456, 0.406],
        std=[0.229, 0.224, 0.225]
    )
])

print("訓練時資料增強:")
print(train_transform)
print("\n測試時預處理:")
print(test_transform)
</code></pre>

            <div class="warning-box">
                <h4>資料增強注意事項</h4>
                <ul>
                    <li><strong>不要過度</strong> - 過度增強可能改變語義（如翻轉數字）</li>
                    <li><strong>測試集不增強</strong> - 評估時應使用原始資料</li>
                    <li><strong>任務相關</strong> - 根據任務選擇合適的增強方式</li>
                    <li><strong>標準化</strong> - 訓練和測試使用相同的標準化參數</li>
                </ul>
            </div>
        </div>

        <div class="content">
            <h2>7. 實戰：CIFAR-10 影像分類</h2>
            
            <h3>載入 CIFAR-10 資料集</h3>
            <pre><code class="language-python">import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
import torchvision
import torchvision.transforms as transforms
import matplotlib.pyplot as plt
import numpy as np

# 資料增強與標準化
transform_train = transforms.Compose([
    transforms.RandomCrop(32, padding=4),
    transforms.RandomHorizontalFlip(),
    transforms.ToTensor(),
    transforms.Normalize((0.4914, 0.4822, 0.4465), 
                        (0.2023, 0.1994, 0.2010))
])

transform_test = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.4914, 0.4822, 0.4465), 
                        (0.2023, 0.1994, 0.2010))
])

# 載入資料集
train_dataset = torchvision.datasets.CIFAR10(
    root='./data', train=True, download=True, transform=transform_train
)

test_dataset = torchvision.datasets.CIFAR10(
    root='./data', train=False, download=True, transform=transform_test
)

# 建立資料載入器
train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=2)
test_loader = DataLoader(test_dataset, batch_size=100, shuffle=False, num_workers=2)

# CIFAR-10 類別
classes = ('plane', 'car', 'bird', 'cat', 'deer', 
           'dog', 'frog', 'horse', 'ship', 'truck')

print(f"訓練集大小: {len(train_dataset)}")
print(f"測試集大小: {len(test_dataset)}")
print(f"類別: {classes}")

# 視覺化部分資料
def imshow(img):
    img = img / 2 + 0.5  # 反標準化
    npimg = img.numpy()
    plt.imshow(np.transpose(npimg, (1, 2, 0)))
    plt.axis('off')

# 獲取一批訓練資料
dataiter = iter(train_loader)
images, labels = next(dataiter)

# 顯示前 16 張影像
fig, axes = plt.subplots(4, 4, figsize=(10, 10))
for i, ax in enumerate(axes.flat):
    if i < 16:
        img = images[i] / 2 + 0.5
        npimg = img.numpy()
        ax.imshow(np.transpose(npimg, (1, 2, 0)))
        ax.set_title(classes[labels[i]])
        ax.axis('off')
plt.tight_layout()
plt.show()
</code></pre>

            <h3>建立並訓練模型</h3>
            <pre><code class="language-python"># 建立 ResNet 模型
model = ResNet([2, 2, 2, 2], num_classes=10)

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = model.to(device)

print(f"使用設備: {device}")
print(f"模型參數量: {sum(p.numel() for p in model.parameters()):,}")

# 定義損失函數和優化器
criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)
scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)

# 訓練函數
def train_epoch(model, train_loader, criterion, optimizer, device):
    model.train()
    train_loss = 0
    correct = 0
    total = 0
    
    for batch_idx, (inputs, targets) in enumerate(train_loader):
        inputs, targets = inputs.to(device), targets.to(device)
        
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, targets)
        loss.backward()
        optimizer.step()
        
        train_loss += loss.item()
        _, predicted = outputs.max(1)
        total += targets.size(0)
        correct += predicted.eq(targets).sum().item()
    
    return train_loss / len(train_loader), 100. * correct / total

# 測試函數
def test(model, test_loader, criterion, device):
    model.eval()
    test_loss = 0
    correct = 0
    total = 0
    
    with torch.no_grad():
        for inputs, targets in test_loader:
            inputs, targets = inputs.to(device), targets.to(device)
            outputs = model(inputs)
            loss = criterion(outputs, targets)
            
            test_loss += loss.item()
            _, predicted = outputs.max(1)
            total += targets.size(0)
            correct += predicted.eq(targets).sum().item()
    
    return test_loss / len(test_loader), 100. * correct / total

# 訓練模型
num_epochs = 20
train_losses = []
train_accs = []
test_losses = []
test_accs = []

print("\n開始訓練...")
for epoch in range(num_epochs):
    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)
    test_loss, test_acc = test(model, test_loader, criterion, device)
    
    train_losses.append(train_loss)
    train_accs.append(train_acc)
    test_losses.append(test_loss)
    test_accs.append(test_acc)
    
    print(f'Epoch: {epoch+1}/{num_epochs}')
    print(f'  Train Loss: {train_loss:.4f}, Acc: {train_acc:.2f}%')
    print(f'  Test Loss: {test_loss:.4f}, Acc: {test_acc:.2f}%')
    
    scheduler.step()

print("\n訓練完成！")
</code></pre>

            <h3>評估與視覺化</h3>
            <pre><code class="language-python"># 繪製訓練曲線
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))

ax1.plot(train_losses, label='訓練損失', linewidth=2)
ax1.plot(test_losses, label='測試損失', linewidth=2)
ax1.set_xlabel('Epoch')
ax1.set_ylabel('損失')
ax1.set_title('訓練與測試損失')
ax1.legend()
ax1.grid(True, alpha=0.3)

ax2.plot(train_accs, label='訓練準確率', linewidth=2)
ax2.plot(test_accs, label='測試準確率', linewidth=2)
ax2.set_xlabel('Epoch')
ax2.set_ylabel('準確率 (%)')
ax2.set_title('訓練與測試準確率')
ax2.legend()
ax2.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# 測試預測
model.eval()
dataiter = iter(test_loader)
images, labels = next(dataiter)

images = images.to(device)
outputs = model(images)
_, predicted = outputs.max(1)

# 視覺化預測結果
fig, axes = plt.subplots(4, 4, figsize=(12, 12))
for i, ax in enumerate(axes.flat):
    if i < 16:
        img = images[i].cpu() / 2 + 0.5
        npimg = img.numpy()
        ax.imshow(np.transpose(npimg, (1, 2, 0)))
        
        true_label = classes[labels[i]]
        pred_label = classes[predicted[i].cpu()]
        color = 'green' if true_label == pred_label else 'red'
        
        ax.set_title(f'真實: {true_label}\n預測: {pred_label}', color=color)
        ax.axis('off')

plt.tight_layout()
plt.show()

# 混淆矩陣
from sklearn.metrics import confusion_matrix
import seaborn as sns

all_preds = []
all_labels = []

model.eval()
with torch.no_grad():
    for inputs, targets in test_loader:
        inputs = inputs.to(device)
        outputs = model(inputs)
        _, predicted = outputs.max(1)
        
        all_preds.extend(predicted.cpu().numpy())
        all_labels.extend(targets.numpy())

cm = confusion_matrix(all_labels, all_preds)

plt.figure(figsize=(10, 8))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=classes, yticklabels=classes)
plt.title('CIFAR-10 混淆矩陣')
plt.ylabel('真實標籤')
plt.xlabel('預測標籤')
plt.show()

# 儲存模型
torch.save(model.state_dict(), 'cifar10_resnet.pth')
print("\n模型已儲存至 cifar10_resnet.pth")
</code></pre>
        </div>

        <div class="content">
            <div class="exercise-section">
                <h3>本週練習題</h3>
                
                <h4>基礎練習（必做）</h4>
                <ol>
                    <li>
                        <strong>從零實現卷積</strong>
                        <ul>
                            <li>純 NumPy 實現卷積運算</li>
                            <li>實作不同的卷積核（邊緣、模糊等）</li>
                            <li>實現池化層</li>
                            <li>在真實影像上測試</li>
                        </ul>
                    </li>
                    
                    <li>
                        <strong>建立簡單 CNN</strong>
                        <ul>
                            <li>使用 PyTorch 建立 CNN</li>
                            <li>在 MNIST 上訓練</li>
                            <li>達到 99% 以上準確率</li>
                            <li>視覺化特徵圖</li>
                        </ul>
                    </li>
                    
                    <li>
                        <strong>資料增強實驗</strong>
                        <ul>
                            <li>實作各種資料增強</li>
                            <li>比較有無資料增強的效果</li>
                            <li>視覺化增強後的影像</li>
                            <li>分析對模型性能的影響</li>
                        </ul>
                    </li>
                </ol>

                <h4>進階練習</h4>
                <ol start="4">
                    <li>
                        <strong>實作經典 CNN 架構</strong>
                        <ul>
                            <li>從零實作 VGG 或 ResNet</li>
                            <li>在 CIFAR-10 上訓練</li>
                            <li>達到 90% 以上測試準確率</li>
                            <li>比較不同架構的效果</li>
                        </ul>
                    </li>
                    
                    <li>
                        <strong>遷移學習應用</strong>
                        <ul>
                            <li>使用預訓練模型</li>
                            <li>在自訂資料集上微調</li>
                            <li>比較不同的微調策略</li>
                            <li>視覺化學習到的特徵</li>
                        </ul>
                    </li>
                </ol>

                <h4>挑戰練習</h4>
                <ol start="6">
                    <li>
                        <strong>視覺化 CNN 內部</strong>
                        <ul>
                            <li>視覺化卷積層的濾波器</li>
                            <li>繪製特徵圖</li>
                            <li>實作 Grad-CAM（類別激活圖）</li>
                            <li>分析模型關注的區域</li>
                        </ul>
                    </li>
                    
                    <li>
                        <strong>Kaggle 競賽</strong>
                        <ul>
                            <li>參加影像分類競賽</li>
                            <li>實驗不同的模型架構</li>
                            <li>使用資料增強和集成學習</li>
                            <li>提升排名</li>
                        </ul>
                    </li>
                </ol>
            </div>
        </div>

        <div class="content">
            <h2>本週總結</h2>
            
            <h3>重點回顧</h3>
            <ul>
                <li>理解卷積運算的原理</li>
                <li>掌握卷積層和池化層</li>
                <li>熟悉經典 CNN 架構</li>
                <li>學會使用遷移學習</li>
                <li>掌握資料增強技術</li>
                <li>完成 CIFAR-10 影像分類</li>
            </ul>

            <h3>關鍵概念</h3>
            <ul>
                <li><strong>參數共享</strong> - CNN 比全連接網路參數少很多</li>
                <li><strong>局部連接</strong> - 每個神經元只看局部區域</li>
                <li><strong>殘差連接</strong> - 解決深層網路訓練問題</li>
                <li><strong>遷移學習</strong> - 利用預訓練模型加速訓練</li>
                <li><strong>資料增強</strong> - 防止過擬合的有效方法</li>
            </ul>

            <h3>下週預告</h3>
            <div class="note-box">
                <h4>Week 6 - 循環神經網路（RNN）</h4>
                <p>下週將學習處理序列資料的神經網路：</p>
                <ul>
                    <li>RNN 基本原理與架構</li>
                    <li>LSTM 與 GRU 詳解</li>
                    <li>序列到序列模型（Seq2Seq）</li>
                    <li>注意力機制（Attention）</li>
                    <li>實戰：文本生成與機器翻譯</li>
                </ul>
            </div>
        </div>

        <div class="nav-buttons">
            <a href="llm-week4.html" class="btn btn-secondary">Week 4：深度學習基礎</a>
            <a href="llm-week6.html" class="btn">Week 6：循環神經網路</a>
        </div>

        <footer>
            <p>LLM 與 AI/ML 課程 Week 5</p>
            <p>卷積神經網路 · CNN 架構 · 遷移學習 · 資料增強 · CIFAR-10</p>
            <p style="font-size: 0.9em; color: #95a5a6;">© 2025 All Rights Reserved</p>
        </footer>
    </div>
</body>
</html>